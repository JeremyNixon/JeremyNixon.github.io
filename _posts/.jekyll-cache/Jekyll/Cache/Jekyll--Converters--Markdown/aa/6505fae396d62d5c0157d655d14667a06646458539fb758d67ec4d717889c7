I",<p>By Jeremy Nixon [<a href="mailto:jnixon2@gmail.com">jnixon2@gmail.com</a>]. Nov. 2017.
Categories: Domain in which the paperâ€™s innovation is novel.</p>
<ol>
  <li>Reinforcement Learning
    <ul>
      <li>Multi-Agent</li>
      <li>Exploration</li>
      <li>Imitation Learning</li>
    </ul>
  </li>
  <li>Deep Learning</li>
  <li>Memory</li>
  <li>Program Learning</li>
  <li>Representation Learning</li>
  <li>Variational Inference</li>
  <li>Generative Models</li>
  <li>Evolution</li>
  <li>Applications
    <ul>
      <li>Security / Safety</li>
      <li>Robotics</li>
    </ul>
  </li>
  <li>
    <p>Environments</p>
  </li>
  <li>Reinforcement Learning
    <ul>
      <li>Multi-Agent
        <ul>
          <li><a href="https://arxiv.org/abs/1709.04326">Learning with Opponent-Learning Awareness</a></li>
          <li><a href="https://arxiv.org/abs/1706.02275">Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments</a></li>
          <li><a href="https://arxiv.org/abs/1703.04908">Emergence of Grounded Compositional Language in Multi-Agent Populations</a></li>
        </ul>
      </li>
      <li>Exploration
        <ul>
          <li><a href="https://arxiv.org/abs/1706.01905">Parameter Space Noise for Exploration</a></li>
          <li><a href="https://arxiv.org/abs/1706.01502">UCB and InfoGain Exploration via Q-Ensembles</a></li>
          <li><a href="https://arxiv.org/abs/1611.04717">Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning</a></li>
          <li><a href="https://arxiv.org/abs/1605.09674">VIME: Variational Information Maximizing Exploration</a></li>
        </ul>
      </li>
      <li>Imitation Learning
        <ul>
          <li><a href="https://arxiv.org/abs/1703.01703">Third-Person Imitation Learning</a></li>
          <li><a href="https://arxiv.org/abs/1703.07326">One-Shot Imitation Learning</a></li>
        </ul>
      </li>
      <li><a href="https://arxiv.org/abs/1611.02779">RL2: Fast Reinforcement Learning via Slow Reinforcement Learning</a></li>
      <li><a href="https://arxiv.org/abs/1707.00183">Teacher-Student Curriculum Learning</a></li>
      <li><a href="https://arxiv.org/abs/1704.06440">Equivalence Between Policy Gradients and Soft Q-Learning</a></li>
      <li><a href="https://arxiv.org/abs/1703.04070">Prediction and Control with Temporal Segment Models</a></li>
    </ul>
  </li>
  <li>Deep Learning
    <ul>
      <li><a href="https://arxiv.org/abs/1602.07868">Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</a></li>
    </ul>
  </li>
  <li>Memory
    <ul>
      <li><a href="https://arxiv.org/pdf/1707.01495.pdf">Hindsight Experience Replay [Also, Reinforcement Learning]</a></li>
    </ul>
  </li>
  <li>Program Learning
    <ul>
      <li><a href="https://arxiv.org/abs/1611.00736">Extensions and Limitations of the Neural GPU</a></li>
    </ul>
  </li>
  <li>Representation Learning
    <ul>
      <li><a href="https://arxiv.org/abs/1611.02731">Variational Lossy Autoencoder</a></li>
    </ul>
  </li>
  <li>Variational Inference
    <ul>
      <li><a href="https://arxiv.org/abs/1606.04934">Improving Variational Inference with Inverse Autoregressive Flow</a></li>
    </ul>
  </li>
  <li>Generative Models
    <ul>
      <li>Generative Adversarial Networks
        <ul>
          <li><a href="https://arxiv.org/abs/1606.03657">InfoGan: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets [Also, Representation Learning]</a></li>
          <li><a href="https://arxiv.org/abs/1606.03498">Improved Techniques for Training GANs</a></li>
        </ul>
      </li>
      <li><a href="https://arxiv.org/abs/1611.04273">On the Quantitative Analysis of Decoder-Based Generative Models</a></li>
      <li><a href="https://arxiv.org/pdf/1611.03852.pdf">A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy Based Models [Also Reinforcement Learning]</a></li>
      <li><a href="https://arxiv.org/abs/1701.05517">PixelCNN++: Improving the Pixel CNN with Discretized Logistic Mixture Likelihood and Other Modifications</a></li>
      <li><a href="https://arxiv.org/abs/1704.01444">Learning to Generate Reviews and Discovering Sentiment</a></li>
    </ul>
  </li>
  <li>Evolution
    <ul>
      <li><a href="https://arxiv.org/abs/1703.03864">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a></li>
    </ul>
  </li>
  <li>Applications
    <ul>
      <li>Security / Safety
        <ul>
          <li><a href="https://arxiv.org/abs/1706.03741">Deep Reinforcement Learning from Human Preferences</a></li>
          <li><a href="https://arxiv.org/abs/1606.06565">Concrete Problems in AI Safety</a></li>
          <li><a href="https://arxiv.org/abs/1702.02284">Adversarial Attacks on Neural Network Policies</a></li>
          <li><a href="https://arxiv.org/abs/1605.07725">Adversarial Training Methods for Semi-Supervised Text Classification</a></li>
          <li><a href="https://arxiv.org/abs/1610.05755">Semi-Supervised Knowledge Transfer for Deep Learning from Private Training Data</a></li>
          <li><a href="https://arxiv.org/pdf/1805.00899.pdf">Debate Amplification</a></li>
        </ul>
      </li>
      <li>Robotics
        <ul>
          <li><a href="https://arxiv.org/abs/1703.06907">Domain Randomization for Transferring Deep NEural Networks from Simulation to the Real World</a></li>
          <li><a href="https://arxiv.org/abs/1610.03518">Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Environments
    <ul>
      <li><a href="https://blog.openai.com/infrastructure-for-deep-learning/">Infrastructure for Deep Learning</a></li>
      <li><a href="https://blog.openai.com/universe/">Universe</a></li>
      <li><a href="https://arxiv.org/abs/1606.01540">OpenAI Gym</a></li>
    </ul>
  </li>
</ol>

<p>OpenAI Researchers</p>
<ol>
  <li>Paul Christiano</li>
  <li><del>Ryan Lowe</del></li>
  <li><del>Jean Harb</del></li>
  <li><del>Pieter Abbeel</del></li>
  <li><del>Igor Mordatch</del></li>
  <li>Matthias Plappert</li>
  <li><del>Rein Houthooft</del></li>
  <li>Prafulla Dhariwal</li>
  <li>Szymon Sidor</li>
  <li>Richard Y. Chen</li>
  <li><del>Xi Chen</del></li>
  <li><del>Marcin Andrychowicz</del></li>
  <li>John Schulman</li>
  <li>Alec Radford</li>
  <li><del>Rafal Jozefowicz</del></li>
  <li><del>Yan Duan</del></li>
  <li><del>Bradly C. Stadie</del></li>
  <li><del>Jonathan Ho</del></li>
  <li>Jonas Schneider</li>
  <li>Ilya Sutskever</li>
  <li>Wojciech Zaremba</li>
  <li><del>Rachel Fong</del></li>
  <li>Josh Tobin</li>
  <li>Alex Ray</li>
  <li><del>Nikhil Mishra</del></li>
  <li><del>Ian Goodfellow</del></li>
  <li><del>Tim Salimans</del></li>
  <li><del>Diederik P. Kingma</del></li>
  <li><del>Andrej Karpathy</del></li>
  <li><del>Yuri Burda</del></li>
  <li><del>Zain Shah</del></li>
  <li><del>Trevor Blackwell</del></li>
  <li><del>Vicki Cheung</del></li>
</ol>

<p><a href="http://990s.foundationcenter.org/990_pdf_archive/810/810861541/810861541_201612_990.pdf">Salaries of top employees</a> [Pg. 28]
<a href="http://www.guidestar.org/FinDocuments/2016/810/861/2016-810861541-0eb61629-9.pdf">Hours &amp; Salaries of top employees</a> [Pg. 7]
OpenAI spent 11 million in 2016, 7 million on salary. For comparison, Deepmind spend 138 million in 2016.</p>

:ET