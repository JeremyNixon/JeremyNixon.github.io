<p>By Jeremy Nixon [<a href="mailto:jnixon2@gmail.com">jnixon2@gmail.com</a>]. Nov 2017.</p>

<p>Categories: Domain in which the paperâ€™s innovation is novel.</p>

<ol>
  <li>
    <p>Reinforcement Learning</p>

    <ol>
      <li>
        <p>Multi-Agent</p>
      </li>
      <li>
        <p>Exploration</p>
      </li>
      <li>
        <p>Imitation Learning</p>
      </li>
    </ol>
  </li>
  <li>
    <p>Deep Learning</p>
  </li>
  <li>
    <p>Memory</p>
  </li>
  <li>
    <p>Program Learning</p>
  </li>
  <li>
    <p>Representation Learning</p>
  </li>
  <li>
    <p>Variational Inference</p>
  </li>
  <li>
    <p>Generative Models</p>
  </li>
  <li>
    <p>Evolution</p>
  </li>
  <li>
    <p>Applications</p>

    <ol>
      <li>
        <p>Security / Safety</p>
      </li>
      <li>
        <p>Robotics</p>
      </li>
    </ol>
  </li>
  <li>
    <p>Environments</p>
  </li>
</ol>

<p>Papers:</p>

<ol>
  <li>
    <p>Reinforcement Learning</p>

    <ol>
      <li>
        <p>Multi-Agent</p>

        <ol>
          <li>
            <p>Learning with Opponent-Learning Awareness</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1709.04326">https://arxiv.org/abs/1709.04326</a></p>
          </li>
          <li>
            <p>Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1706.02275">https://arxiv.org/abs/1706.02275</a></p>
          </li>
          <li>
            <p>Emergence of Grounded Compositional Language in Multi-Agent Populations</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1703.04908">https://arxiv.org/abs/1703.04908</a></p>
          </li>
        </ol>
      </li>
      <li>
        <p>Exploration</p>

        <ol>
          <li>
            <p>Parameter Space Noise for Exploration</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1706.01905">https://arxiv.org/abs/1706.01905</a></p>
          </li>
          <li>
            <p>UCB and InfoGain Exploration via Q-Ensembles</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1706.01502">https://arxiv.org/abs/1706.01502</a></p>
          </li>
          <li>
            <p>Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1611.04717">https://arxiv.org/abs/1611.04717</a></p>
          </li>
          <li>
            <p>VIME: Variational Information Maximizing Exploration</p>
          </li>
          <li>
            <p>https://arxiv.org/abs/1605.09674</p>
          </li>
        </ol>
      </li>
      <li>
        <p>Imitation Learning</p>

        <ol>
          <li>
            <p>Third-Person Imitation Learning</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1703.01703">https://arxiv.org/abs/1703.01703</a></p>
          </li>
          <li>
            <p>One-Shot Imitation Learning</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1703.07326">https://arxiv.org/abs/1703.07326</a></p>
          </li>
        </ol>
      </li>
      <li>
        <p>RL2: Fast Reinforcement Learning via Slow Reinforcement Learning</p>
      </li>
      <li>
        <p>https://arxiv.org/abs/1611.02779</p>
      </li>
      <li>
        <p>Teacher-Student Curriculum Learning</p>
      </li>
      <li>
        <p><a href="https://arxiv.org/abs/1707.00183">https://arxiv.org/abs/1707.00183</a></p>
      </li>
      <li>
        <p>Equivalence Between Policy Gradients and Soft Q-Learning</p>
      </li>
      <li>
        <p><a href="https://arxiv.org/abs/1704.06440">https://arxiv.org/abs/1704.06440</a></p>
      </li>
      <li>
        <p>Prediction and Control with Temporal Segment Models</p>
      </li>
      <li>
        <p>https://arxiv.org/abs/1703.04070</p>
      </li>
    </ol>
  </li>
  <li>
    <p>Deep Learning</p>

    <ol>
      <li>
        <p>Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</p>
      </li>
      <li>
        <p>https://arxiv.org/abs/1602.07868</p>
      </li>
    </ol>
  </li>
  <li>
    <p>Memory</p>

    <ol>
      <li>
        <p>Hindsight Experience Replay [Also, Reinforcement Learning]</p>
      </li>
      <li>
        <p><a href="https://arxiv.org/pdf/1707.01495.pdf">https://arxiv.org/pdf/1707.01495.pdf</a></p>
      </li>
    </ol>
  </li>
  <li>
    <p>Program Learning</p>

    <ol>
      <li>
        <p>Extensions and Limitations of the Neural GPU</p>
      </li>
      <li>
        <p>https://arxiv.org/abs/1611.00736</p>
      </li>
    </ol>
  </li>
  <li>
    <p>Representation Learning</p>

    <ol>
      <li>
        <p>Variational Lossy Autoencoder</p>
      </li>
      <li>
        <p><a href="https://arxiv.org/abs/1611.02731">https://arxiv.org/abs/1611.02731</a></p>
      </li>
    </ol>
  </li>
  <li>
    <p>Variational Inference</p>

    <ol>
      <li>
        <p>Improving Variational Inference with Inverse Autoregressive Flow</p>
      </li>
      <li>
        <p>https://arxiv.org/abs/1606.04934</p>
      </li>
    </ol>
  </li>
  <li>
    <p>Generative Models</p>

    <ol>
      <li>
        <p>Generative Adversarial Networks</p>

        <ol>
          <li>
            <p>InfoGan: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets [Also, Representation Learning]</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1606.03657">https://arxiv.org/abs/1606.03657</a></p>
          </li>
          <li>
            <p>Improved Techniques for Training GANs</p>
          </li>
          <li>
            <p>https://arxiv.org/abs/1606.03498</p>
          </li>
        </ol>
      </li>
      <li>
        <p>On the Quantitative Analysis of Decoder-Based Generative Models</p>
      </li>
      <li>
        <p><a href="https://arxiv.org/abs/1611.04273">https://arxiv.org/abs/1611.04273</a></p>
      </li>
      <li>
        <p>A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy Based Models [Also Reinforcement Learning]</p>
      </li>
      <li>
        <p><a href="https://arxiv.org/pdf/1611.03852.pdf">https://arxiv.org/pdf/1611.03852.pdf</a></p>
      </li>
      <li>
        <p>PixelCNN++: Improving the Pixel CNN with Discretized Logistic Mixture Likelihood and Other Modifications</p>
      </li>
      <li>
        <p><a href="https://arxiv.org/abs/1701.05517">https://arxiv.org/abs/1701.05517</a></p>
      </li>
      <li>
        <p>Learning to Generate Reviews and Discovering Sentiment</p>
      </li>
      <li>
        <p><a href="https://arxiv.org/abs/1704.01444">https://arxiv.org/abs/1704.01444</a></p>
      </li>
    </ol>
  </li>
  <li>
    <p>Evolution</p>

    <ol>
      <li>
        <p>Evolution Strategies as a Scalable Alternative to Reinforcement Learning</p>
      </li>
      <li>
        <p>https://arxiv.org/abs/1703.03864</p>
      </li>
    </ol>
  </li>
  <li>
    <p>Applications</p>

    <ol>
      <li>
        <p>Security / Safety</p>

        <ol>
          <li>
            <p>Deep Reinforcement Learning from Human Preferences</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1706.03741">https://arxiv.org/abs/1706.03741</a></p>
          </li>
          <li>
            <p>Concrete Problems in AI Safety</p>
          </li>
          <li>
            <p>https://arxiv.org/abs/1606.06565</p>
          </li>
          <li>
            <p>Adversarial Attacks on Neural Network Policies</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1702.02284">https://arxiv.org/abs/1702.02284</a></p>
          </li>
          <li>
            <p>Adversarial Training Methods for Semi-Supervised Text Classification</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1605.07725">https://arxiv.org/abs/1605.07725</a></p>
          </li>
          <li>
            <p>Semi-Supervised Knowledge Transfer for Deep Learning from Private Training Data</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1610.05755">https://arxiv.org/abs/1610.05755</a></p>
          </li>
          <li>
            <p>Debate Amplification</p>
          </li>
          <li>
            <p>https://arxiv.org/pdf/1805.00899.pdf</p>
          </li>
        </ol>
      </li>
      <li>
        <p>Robotics</p>

        <ol>
          <li>
            <p>Domain Randomization for Transferring Deep NEural Networks from Simulation to the Real World</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1703.06907">https://arxiv.org/abs/1703.06907</a></p>
          </li>
          <li>
            <p>Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1610.03518">https://arxiv.org/abs/1610.03518</a></p>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>
    <p>Environments</p>

    <ol>
      <li>
        <p>Infrastructure for Deep Learning</p>
      </li>
      <li>
        <p>https://blog.openai.com/infrastructure-for-deep-learning/</p>
      </li>
      <li>
        <p>Universe</p>
      </li>
      <li>
        <p><a href="https://blog.openai.com/universe/">https://blog.openai.com/universe/</a></p>
      </li>
      <li>
        <p>OpenAI Gym</p>
      </li>
      <li>
        <p><a href="https://arxiv.org/abs/1606.01540">https://arxiv.org/abs/1606.01540</a></p>
      </li>
    </ol>
  </li>
</ol>

<p>OpenAI Researchers (Every name on a published OpenAI Paper)</p>

<p>Paul Christiano</p>

<p><del>Ryan Lowe</del></p>

<p><del>Jean Harb</del></p>

<p><del>Pieter Abbeel</del></p>

<p>Igor Mordatch</p>

<p>Matthias Plappert</p>

<p>Rein Houthooft</p>

<p>Prafulla Dhariwal</p>

<p>Szymon Sidor</p>

<p>Richard Y. Chen</p>

<p><del>Xi Chen</del></p>

<p>Marcin Andrychowicz</p>

<p>John Schulman</p>

<p>Alec Radford</p>

<p><del>Rafal Jozefowicz</del></p>

<p><del>Yan Duan</del></p>

<p><del>Bradly C. Stadie</del></p>

<p><del>Jonathan Ho</del></p>

<p>Jonas Schneider</p>

<p>Ilya Sutskever</p>

<p>Wojciech Zaremba</p>

<p><del>Rachel Fong</del></p>

<p>Josh Tobin</p>

<p>Alex Ray</p>

<p><del>Nikhil Mishra</del></p>

<p><del>Ian Goodfellow</del></p>

<p><del>Tim Salimans</del></p>

<p><del>Diederik P. Kingma</del></p>

<p><del>Andrej Karpathy</del></p>

<p>Yuri Burda</p>

<p><del>Zain Shah</del></p>

<p><del>Trevor Blackwell</del></p>

<p><del>Vicki Cheung</del></p>

<p><a href="http://990s.foundationcenter.org/990_pdf_archive/810/810861541/810861541_201612_990.pdf">Salaries of top employees</a> [Pg. 28]</p>

<p><a href="http://www.guidestar.org/FinDocuments/2016/810/861/2016-810861541-0eb61629-9.pdf">Hours &amp; Salaries of top employees</a> [Pg. 7]</p>

<p>OpenAI spent 11 million in 2016, 7 million on salary. For comparison, Deepmind spend 138 million in 2016.</p>
