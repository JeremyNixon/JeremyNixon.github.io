---
layout: post
title:  "19 Recent Breakthroughs in Deep Learning"
date: 2019-05-02
categories: intelligence
---
1. Residual Layers
     * [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
     * [Highway Networks](https://arxiv.org/abs/1505.00387) (Simultaneity in Research)
2. Attention
     * [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)
     * [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
     * Transformer
          * [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
3. Batch Normalization
     * [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
4. Sequence-to-Sequence
     * [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
5. [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)
     * Episodic Memory
6. Distributional Reinforcement Learning
     * [A Distributional Perspective on Reinforcement Learning
](https://arxiv.org/abs/1707.06887)
7. Region Proposal
     * [Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf)
     * [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497.pdf)
8. Word Embeddings
     * [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)
9. Contextual Word Embeddings (ELMo)
     * [Deep Contextualized Word Representations](https://arxiv.org/abs/1802.05365)
10. Large scale language modeling
     * Masked
          * [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) (BERT, Bi-directional Encoder Representations for Transformer)
     * Forward Prediction
          * [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (GPT, Generative Pre-Trained Transformer)
11. MAML
     * [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400)
12. Adversarial Examples
     * [Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199)
13. Rethinking Generalization
     * [Understanding Deep Learning Requires Rethinking Generalization](https://arxiv.org/abs/1611.03530)
14. Adaptive Optimizers
     * Momentum
          * [Some Methods of Speeding up the Convergence of Iteration Methods](https://www.researchgate.net/publication/243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods)
          * [On the Importance of Initialization and Momentum in Deep Learning
](http://proceedings.mlr.press/v28/sutskever13.html)
     * RMSProp
     * Adam
          * [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)
15. GAN
     * [Generative Adversarial Nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)
16. Variational Inference
     * VAE
         * [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)
17. AutoGrad / Automatic Differentiation
     * [Automatic Differentiation in Machine Learning: a Survey](http://www.jmlr.org/papers/volume18/17-468/17-468.pdf)
18. Style Transfer
     * [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576)
19. Older Breakthroughs
     * Convolution / CNN
     * Recurrence / RNN
          * Gating / LSTM
     * Backpropagation





